{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Sample text dataset\n",
        "text = \"hello world this is a simple text generation example using LSTM.\"\n",
        "chars = sorted(set(text))  # Unique characters\n",
        "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "idx_to_char = {i: ch for ch, i in char_to_idx.items()}\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Convert text to sequences\n",
        "def text_to_sequence(text, seq_length):\n",
        "    inputs, targets = [], []\n",
        "    for i in range(len(text) - seq_length):\n",
        "        inputs.append([char_to_idx[ch] for ch in text[i:i+seq_length]])\n",
        "        targets.append(char_to_idx[text[i+seq_length]])\n",
        "    return torch.tensor(inputs), torch.tensor(targets)\n",
        "\n",
        "seq_length = 10\n",
        "X, Y = text_to_sequence(text, seq_length)\n",
        "\n",
        "# Define RNN model\n",
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers):\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_dim)  # Convert to embeddings\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embed(x)  # Convert to embeddings\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        out = self.fc(out[:, -1, :])  # Predict last character in sequence\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim = 16\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "learning_rate = 0.005\n",
        "num_epochs = 500\n",
        "batch_size = X.shape[0]\n",
        "teacher_forcing_ratio = 0.5  # Teacher forcing probability\n",
        "\n",
        "# Model, loss function, optimizer\n",
        "model = CharRNN(vocab_size, embedding_dim, hidden_size, num_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop with Teacher Forcing\n",
        "for epoch in range(num_epochs):\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Use real target as input (Teacher Forcing)\n",
        "        outputs, _ = model(X, hidden)\n",
        "    else:\n",
        "        # Free Running Mode (use predicted output as next input)\n",
        "        outputs = []\n",
        "        input_seq = X[:, 0].unsqueeze(1)  # Start with first char in each batch\n",
        "        for _ in range(seq_length):\n",
        "            output, hidden = model(input_seq, hidden)\n",
        "            outputs.append(output)\n",
        "            input_seq = torch.argmax(output, dim=1).unsqueeze(1)  # Next input = predicted char\n",
        "        outputs = torch.stack(outputs, dim=1)[:, -1, :]  # Take last prediction\n",
        "\n",
        "    loss = criterion(outputs, Y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Text generation function with sampling\n",
        "def generate_text(start_text, length=100, temperature=1.0, greedy=False):\n",
        "    model.eval()\n",
        "    input_seq = torch.tensor([[char_to_idx[ch] for ch in start_text]])\n",
        "    hidden = model.init_hidden(1)\n",
        "\n",
        "    result = start_text\n",
        "    for _ in range(length):\n",
        "        with torch.no_grad():\n",
        "            output, hidden = model(input_seq, hidden)\n",
        "            if greedy:\n",
        "                predicted_idx = torch.argmax(output).item()  # Greedy search\n",
        "            else:\n",
        "                probabilities = torch.softmax(output / temperature, dim=1)\n",
        "                predicted_idx = torch.multinomial(probabilities, 1).item()  # Sampling\n",
        "\n",
        "            result += idx_to_char[predicted_idx]\n",
        "            input_seq = torch.tensor([[predicted_idx]])\n",
        "\n",
        "    return result\n",
        "\n",
        "# Generate text with greedy search and temperature sampling\n",
        "print(\"Greedy Search Output:\")\n",
        "print(generate_text(\"hello\", 100, greedy=True))\n",
        "\n",
        "print(\"\\nTemperature Sampling (T=0.8):\")\n",
        "print(generate_text(\"hello\", 100, temperature=0.8, greedy=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o3XxPWRTJSu",
        "outputId": "03c7cb5a-ef37-41d0-d5f2-71455b57c8c4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/500], Loss: 1.4572\n",
            "Epoch [100/500], Loss: 0.2569\n",
            "Epoch [150/500], Loss: 1.5625\n",
            "Epoch [200/500], Loss: 0.0174\n",
            "Epoch [250/500], Loss: 1.2188\n",
            "Epoch [300/500], Loss: 0.0040\n",
            "Epoch [350/500], Loss: 1.2114\n",
            "Epoch [400/500], Loss: 0.0074\n",
            "Epoch [450/500], Loss: 1.4155\n",
            "Epoch [500/500], Loss: 0.0057\n",
            "Greedy Search Output:\n",
            "hellodddiiiiiiddiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii\n",
            "\n",
            "Temperature Sampling (T=0.8):\n",
            "hellodddidiiiiiiidiiiiiiiiiiiiiiiiiiii iii iini  iiiiiii iiiiininniiiiiiniiiiniinniiiiiiiiiiii iiii iiiii\n"
          ]
        }
      ]
    }
  ]
}