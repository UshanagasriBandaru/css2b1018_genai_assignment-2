{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import classification_report\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "import os\n",
        "\n",
        "# Ensure the GloVe file exists\n",
        "glove_path = \"/content/glove.6B.50d.txt\"\n",
        "if not os.path.exists(glove_path):\n",
        "    raise FileNotFoundError(f\"GloVe file not found: {glove_path}. Place it in the correct directory.\")\n",
        "\n",
        "# Load pretrained GloVe embeddings\n",
        "def load_glove_embeddings(glove_path, word_index, embedding_dim=50):\n",
        "    embeddings = np.random.randn(len(word_index), embedding_dim) * 0.01  # Small random initialization\n",
        "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype=np.float32)\n",
        "            if word in word_index:\n",
        "                embeddings[word_index[word]] = vector\n",
        "    return torch.tensor(embeddings, dtype=torch.float32)\n",
        "\n",
        "# Dataset class for Named Entity Recognition\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, labels, word2idx, tag2idx):\n",
        "        self.sentences = [[word2idx.get(w, 1) for w in sent] for sent in sentences]  # 1 for <UNK>\n",
        "        self.labels = [[tag2idx[t] for t in label] for label in labels]\n",
        "        self.lengths = [len(sent) for sent in self.sentences]\n",
        "\n",
        "        self.sentences = [torch.tensor(s, dtype=torch.long) for s in self.sentences]\n",
        "        self.labels = [torch.tensor(l, dtype=torch.long) for l in self.labels]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sentences[idx], self.labels[idx], self.lengths[idx]\n",
        "\n",
        "# Collate function for variable-length sequences\n",
        "def collate_fn(batch):\n",
        "    sentences, labels, lengths = zip(*batch)\n",
        "    sentences_padded = pad_sequence(sentences, batch_first=True, padding_value=0)\n",
        "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-1)  # -1 for masked loss\n",
        "    return sentences_padded, labels_padded, torch.tensor(lengths, dtype=torch.long)\n",
        "\n",
        "# BiLSTM Model for Named Entity Recognition\n",
        "class BiLSTM_NER(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, tagset_size, pretrained_embeddings):\n",
        "        super(BiLSTM_NER, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, tagset_size)\n",
        "\n",
        "    def forward(self, sentences, lengths):\n",
        "        embeds = self.embedding(sentences)\n",
        "        packed_embeds = pack_padded_sequence(embeds, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_out, _ = self.lstm(packed_embeds)\n",
        "        lstm_out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
        "        logits = self.fc(lstm_out)\n",
        "        return logits\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, optimizer, loss_fn, device, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for sentences, labels, lengths in train_loader:\n",
        "            sentences, labels, lengths = sentences.to(device), labels.to(device), lengths.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(sentences, lengths)\n",
        "            logits = logits.view(-1, logits.shape[-1])\n",
        "            labels = labels.view(-1)\n",
        "            loss = loss_fn(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# Evaluation function\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model, val_loader, idx2tag, device):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    for sentences, labels, lengths in val_loader:\n",
        "        sentences, labels, lengths = sentences.to(device), labels.to(device), lengths.to(device)\n",
        "        logits = model(sentences, lengths)\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        for i in range(len(lengths)):\n",
        "            all_preds.extend(preds[i][:lengths[i]].cpu().numpy())\n",
        "            all_labels.extend(labels[i][:lengths[i]].cpu().numpy())\n",
        "\n",
        "    # Fix UndefinedMetricWarning by setting zero_division=0\n",
        "    print(classification_report(\n",
        "        all_labels, all_preds,\n",
        "        target_names=list(idx2tag.values()),\n",
        "        labels=list(idx2tag.keys()),\n",
        "        zero_division=0  # This ensures no undefined metric warnings\n",
        "    ))\n",
        "\n",
        "# Main script\n",
        "if __name__ == \"__main__\":\n",
        "    # Sample training data\n",
        "    train_sentences = [['John', 'lives', 'in', 'New', 'York']]\n",
        "    train_labels = [['B-PER', 'O', 'O', 'B-LOC', 'I-LOC']]\n",
        "\n",
        "    word2idx = {'<PAD>': 0, '<UNK>': 1, 'John': 2, 'lives': 3, 'in': 4, 'New': 5, 'York': 6}\n",
        "    tag2idx = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-LOC': 3, 'I-LOC': 4}\n",
        "    idx2tag = {v: k for k, v in tag2idx.items()}  # Reverse mapping for evaluation\n",
        "\n",
        "    # Create dataset and data loader\n",
        "    train_dataset = NERDataset(train_sentences, train_labels, word2idx, tag2idx)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=2, collate_fn=collate_fn)\n",
        "\n",
        "    # Load GloVe embeddings\n",
        "    pretrained_embeddings = load_glove_embeddings(glove_path, word2idx)\n",
        "\n",
        "    # Model setup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = BiLSTM_NER(len(word2idx), 50, 128, len(tag2idx), pretrained_embeddings).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "    # Train and evaluate\n",
        "    train_model(model, train_loader, optimizer, loss_fn, device, epochs=10)\n",
        "    evaluate_model(model, train_loader, idx2tag, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Evh4E0VGiW",
        "outputId": "e1ddc20d-f1af-47a7-9a1f-c166801089a7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.6052\n",
            "Epoch [2/10], Loss: 1.5691\n",
            "Epoch [3/10], Loss: 1.5336\n",
            "Epoch [4/10], Loss: 1.4982\n",
            "Epoch [5/10], Loss: 1.4624\n",
            "Epoch [6/10], Loss: 1.4258\n",
            "Epoch [7/10], Loss: 1.3881\n",
            "Epoch [8/10], Loss: 1.3488\n",
            "Epoch [9/10], Loss: 1.3076\n",
            "Epoch [10/10], Loss: 1.2642\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           O       0.50      1.00      0.67         2\n",
            "       B-PER       0.00      0.00      0.00         1\n",
            "       I-PER       0.00      0.00      0.00         0\n",
            "       B-LOC       0.00      0.00      0.00         1\n",
            "       I-LOC       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           0.60         5\n",
            "   macro avg       0.30      0.40      0.33         5\n",
            "weighted avg       0.40      0.60      0.47         5\n",
            "\n"
          ]
        }
      ]
    }
  ]
}