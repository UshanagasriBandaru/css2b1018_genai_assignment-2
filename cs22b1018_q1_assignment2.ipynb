{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SB8a7Js6bC4y",
        "outputId": "6e170c70-315c-408e-b280-dabd2e9a4bfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outputs: [array([[-0.13247714],\n",
            "       [ 0.14764116]]), array([[0.217034  ],\n",
            "       [0.38420155]]), array([[ 0.07121305],\n",
            "       [-0.01310297]]), array([[0.10845563],\n",
            "       [0.31901473]])]\n",
            "Final hidden state: [[ 0.61189645]\n",
            " [-0.00710323]\n",
            " [ 0.38153264]\n",
            " [ 0.78761046]\n",
            " [ 0.87324151]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class CustomRNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size, alpha=0.01):\n",
        "        self.hidden_size = hidden_size\n",
        "        self.alpha = alpha  # Leaky ReLU parameter\n",
        "\n",
        "        # Xavier Initialization for stability\n",
        "        self.W_x = np.random.randn(hidden_size, input_size) * np.sqrt(1 / input_size)\n",
        "        self.W_h = np.random.randn(hidden_size, hidden_size) * np.sqrt(1 / hidden_size)\n",
        "        self.W_y = np.random.randn(output_size, hidden_size) * np.sqrt(1 / hidden_size)\n",
        "\n",
        "        self.b_h = np.zeros((hidden_size, 1))\n",
        "        self.b_y = np.zeros((output_size, 1))\n",
        "\n",
        "    def leaky_relu(self, x):\n",
        "        return np.maximum(self.alpha * x, x)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        T = len(inputs)\n",
        "        h = np.zeros((self.hidden_size, 1))\n",
        "        outputs = []\n",
        "\n",
        "        for t in range(T):\n",
        "            x = inputs[t].reshape(-1, 1)\n",
        "            h = self.leaky_relu(np.dot(self.W_x, x) + np.dot(self.W_h, h) + self.b_h)  # Custom activation\n",
        "            y = np.dot(self.W_y, h) + self.b_y\n",
        "            outputs.append(y)\n",
        "\n",
        "        return outputs, h\n",
        "\n",
        "# Example usage\n",
        "input_size = 3\n",
        "hidden_size = 5\n",
        "output_size = 2\n",
        "\n",
        "rnn = CustomRNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Batch input (T=4, input_size=3)\n",
        "inputs = [np.random.randn(input_size) for _ in range(4)]\n",
        "\n",
        "outputs, final_hidden_state = rnn.forward(inputs)\n",
        "\n",
        "print(\"Outputs:\", outputs)\n",
        "print(\"Final hidden state:\", final_hidden_state)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class VanillaRNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Xavier Initialization\n",
        "        self.W_x = np.random.randn(hidden_size, input_size) * np.sqrt(1. / input_size)\n",
        "        self.W_h = np.random.randn(hidden_size, hidden_size) * np.sqrt(1. / hidden_size)\n",
        "        self.W_y = np.random.randn(output_size, hidden_size) * np.sqrt(1. / hidden_size)\n",
        "\n",
        "        self.b_h = np.zeros((hidden_size, 1))\n",
        "        self.b_y = np.zeros((output_size, 1))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        T = len(inputs)\n",
        "        h_t = np.zeros((self.hidden_size, 1))  # Initial hidden state\n",
        "        hs, ys = {}, {}\n",
        "        hs[-1] = h_t  # Store initial hidden state\n",
        "\n",
        "        for t in range(T):\n",
        "            x_t = inputs[t].reshape(-1, 1)\n",
        "            h_t = np.tanh(np.dot(self.W_x, x_t) + np.dot(self.W_h, hs[t-1]) + self.b_h)\n",
        "            y_t = np.dot(self.W_y, h_t) + self.b_y\n",
        "            hs[t] = h_t\n",
        "            ys[t] = y_t\n",
        "\n",
        "        return ys, hs\n",
        "\n",
        "    def backward(self, inputs, targets, hs, ys):\n",
        "        T = len(inputs)\n",
        "\n",
        "        # Initialize gradients\n",
        "        dW_x = np.zeros_like(self.W_x)\n",
        "        dW_h = np.zeros_like(self.W_h)\n",
        "        dW_y = np.zeros_like(self.W_y)\n",
        "        db_h = np.zeros_like(self.b_h)\n",
        "        db_y = np.zeros_like(self.b_y)\n",
        "        dh_next = np.zeros_like(hs[0])\n",
        "\n",
        "        for t in reversed(range(T)):\n",
        "            dy = ys[t] - targets[t].reshape(-1, 1)  # Error derivative\n",
        "            dW_y += np.dot(dy, hs[t].T)\n",
        "            db_y += dy\n",
        "\n",
        "            dh = np.dot(self.W_y.T, dy) + dh_next\n",
        "            dtanh = (1 - hs[t] ** 2) * dh  # Derivative of tanh\n",
        "\n",
        "            dW_x += np.dot(dtanh, inputs[t].reshape(1, -1))\n",
        "            dW_h += np.dot(dtanh, hs[t-1].T)\n",
        "            db_h += dtanh\n",
        "\n",
        "            dh_next = np.dot(self.W_h.T, dtanh)\n",
        "\n",
        "        # Gradient Clipping (to prevent exploding gradients)\n",
        "        for dparam in [dW_x, dW_h, dW_y, db_h, db_y]:\n",
        "            np.clip(dparam, -1, 1, out=dparam)\n",
        "\n",
        "        # Update weights\n",
        "        self.W_x -= self.learning_rate * dW_x\n",
        "        self.W_h -= self.learning_rate * dW_h\n",
        "        self.W_y -= self.learning_rate * dW_y\n",
        "        self.b_h -= self.learning_rate * db_h\n",
        "        self.b_y -= self.learning_rate * db_y\n",
        "\n",
        "    def train(self, inputs, targets):\n",
        "        ys, hs = self.forward(inputs)\n",
        "        self.backward(inputs, targets, hs, ys)\n",
        "        return ys\n",
        "\n",
        "# Example Usage\n",
        "input_size, hidden_size, output_size = 3, 5, 2\n",
        "rnn = VanillaRNN(input_size, hidden_size, output_size)\n",
        "inputs = [np.random.randn(input_size) for _ in range(4)]\n",
        "targets = [np.random.randn(output_size) for _ in range(4)]\n",
        "\n",
        "outputs = rnn.train(inputs, targets)\n",
        "print(\"Outputs:\", outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oG_UR79bczuO",
        "outputId": "8864fcb0-897e-4ea6-bb40-d8aeebb6a779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outputs: {0: array([[-0.38853524],\n",
            "       [ 0.25306185]]), 1: array([[0.62221773],\n",
            "       [0.20157802]]), 2: array([[-0.77206261],\n",
            "       [-0.50142082]]), 3: array([[-0.04477417],\n",
            "       [-0.94373344]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class VanillaRNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Weight matrices (shared across time steps)\n",
        "        self.W_x = np.random.randn(hidden_size, input_size) * 0.01\n",
        "        self.W_h = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "        self.W_y = np.random.randn(output_size, hidden_size) * 0.01\n",
        "\n",
        "        # Bias vectors\n",
        "        self.b_h = np.zeros((hidden_size, 1))\n",
        "        self.b_y = np.zeros((output_size, 1))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        T = len(inputs)\n",
        "        h = np.zeros((self.hidden_size, 1))  # Initialize hidden state\n",
        "        self.h_states = [h]  # Store hidden states for backpropagation\n",
        "        self.inputs = inputs  # Store inputs for backpropagation\n",
        "        outputs = []\n",
        "\n",
        "        for t in range(T):\n",
        "            x = inputs[t].reshape(-1, 1)  # Ensure input is column vector\n",
        "            h = np.tanh(np.dot(self.W_x, x) + np.dot(self.W_h, h) + self.b_h)  # Hidden state update\n",
        "            y = np.dot(self.W_y, h) + self.b_y  # Output computation\n",
        "            outputs.append(y)\n",
        "            self.h_states.append(h)\n",
        "\n",
        "        return outputs, h\n",
        "\n",
        "    def backward(self, doutputs):\n",
        "        T = len(self.inputs)\n",
        "        dW_x, dW_h, dW_y = np.zeros_like(self.W_x), np.zeros_like(self.W_h), np.zeros_like(self.W_y)\n",
        "        db_h, db_y = np.zeros_like(self.b_h), np.zeros_like(self.b_y)\n",
        "        dh_next = np.zeros((self.hidden_size, 1))\n",
        "\n",
        "        for t in reversed(range(T)):\n",
        "            dy = doutputs[t]\n",
        "            dW_y += np.dot(dy, self.h_states[t + 1].T)\n",
        "            db_y += dy\n",
        "\n",
        "            dh = np.dot(self.W_y.T, dy) + dh_next\n",
        "            dh_raw = (1 - self.h_states[t + 1] ** 2) * dh  # Derivative of tanh\n",
        "            dW_x += np.dot(dh_raw, self.inputs[t].reshape(1, -1))\n",
        "            dW_h += np.dot(dh_raw, self.h_states[t].T)\n",
        "            db_h += dh_raw\n",
        "\n",
        "            dh_next = np.dot(self.W_h.T, dh_raw)\n",
        "\n",
        "        for param, dparam in zip([self.W_x, self.W_h, self.W_y, self.b_h, self.b_y],\n",
        "                                 [dW_x, dW_h, dW_y, db_h, db_y]):\n",
        "            np.clip(dparam, -1, 1, out=dparam)  # Gradient clipping\n",
        "            param -= self.learning_rate * dparam\n",
        "\n",
        "    def train(self, inputs, targets):\n",
        "        outputs, _ = self.forward(inputs)\n",
        "        doutputs = [y_pred - y_true for y_pred, y_true in zip(outputs, targets)]\n",
        "        self.backward(doutputs)\n",
        "\n",
        "# Example usage\n",
        "input_size = 3    # Input feature size\n",
        "hidden_size = 5   # Number of hidden neurons\n",
        "output_size = 2   # Output feature size\n",
        "\n",
        "# Initialize RNN\n",
        "rnn = VanillaRNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Define input sequence (T=4, input_size=3)\n",
        "inputs = [np.random.randn(input_size) for _ in range(4)]\n",
        "targets = [np.random.randn(output_size, 1) for _ in range(4)]\n",
        "\n",
        "# Perform training step\n",
        "rnn.train(inputs, targets)\n",
        "\n",
        "print(\"Training completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI-O71OKdCBd",
        "outputId": "0c0c7e8d-eaca-4421-984e-45c90c72e181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class VanillaRNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Weight matrices\n",
        "        self.W_x = np.random.randn(hidden_size, input_size) * 0.01\n",
        "        self.W_h = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "        self.W_y = np.random.randn(output_size, hidden_size) * 0.01\n",
        "\n",
        "        # Bias vectors\n",
        "        self.b_h = np.zeros((hidden_size, 1))\n",
        "        self.b_y = np.zeros((output_size, 1))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        T = len(inputs)\n",
        "        h = np.zeros((self.hidden_size, 1))  # Initialize hidden state\n",
        "        outputs = []\n",
        "        hidden_states = [h]\n",
        "\n",
        "        for t in range(T):\n",
        "            x = inputs[t].reshape(-1, 1)  # Ensure input is column vector\n",
        "            h = np.tanh(np.dot(self.W_x, x) + np.dot(self.W_h, h) + self.b_h)  # Hidden state update\n",
        "            y = np.dot(self.W_y, h) + self.b_y  # Output computation\n",
        "            outputs.append(y)\n",
        "            hidden_states.append(h)\n",
        "\n",
        "        return outputs, hidden_states\n",
        "\n",
        "    def backward(self, inputs, targets, outputs, hidden_states):\n",
        "        T = len(inputs)\n",
        "        dW_x = np.zeros_like(self.W_x)\n",
        "        dW_h = np.zeros_like(self.W_h)\n",
        "        dW_y = np.zeros_like(self.W_y)\n",
        "        db_h = np.zeros_like(self.b_h)\n",
        "        db_y = np.zeros_like(self.b_y)\n",
        "\n",
        "        dh_next = np.zeros((self.hidden_size, 1))\n",
        "\n",
        "        for t in reversed(range(T)):\n",
        "            x = inputs[t].reshape(-1, 1)\n",
        "            h = hidden_states[t + 1]\n",
        "            h_prev = hidden_states[t]\n",
        "\n",
        "            dy = outputs[t] - targets[t].reshape(-1, 1)  # Properly indented\n",
        "\n",
        "            dW_y += np.dot(dy, h.T)\n",
        "            db_y += dy\n",
        "\n",
        "            dh = np.dot(self.W_y.T, dy) + dh_next\n",
        "            dh_raw = (1 - h ** 2) * dh\n",
        "            dW_x += np.dot(dh_raw, x.T)\n",
        "            dW_h += np.dot(dh_raw, h_prev.T)\n",
        "            db_h += dh_raw\n",
        "\n",
        "            dh_next = np.dot(self.W_h.T, dh_raw)\n",
        "\n",
        "        # Update weights\n",
        "        self.W_x -= self.learning_rate * dW_x\n",
        "        self.W_h -= self.learning_rate * dW_h\n",
        "        self.W_y -= self.learning_rate * dW_y\n",
        "        self.b_h -= self.learning_rate * db_h\n",
        "        self.b_y -= self.learning_rate * db_y\n",
        "\n",
        "# Example usage\n",
        "input_size = 3    # Input feature size\n",
        "hidden_size = 5   # Number of hidden neurons\n",
        "output_size = 2   # Output feature size\n",
        "\n",
        "# Initialize RNN\n",
        "rnn = VanillaRNN(input_size, hidden_size, output_size, learning_rate=0.01)\n",
        "\n",
        "# Define input sequence (T=4, input_size=3)\n",
        "inputs = [np.random.randn(input_size) for _ in range(4)]\n",
        "targets = [np.random.randn(output_size) for _ in range(4)]\n",
        "\n",
        "# Perform forward and backward pass\n",
        "outputs, hidden_states = rnn.forward(inputs)\n",
        "rnn.backward(inputs, targets, outputs, hidden_states)\n",
        "\n",
        "print(\"Outputs:\", outputs)\n",
        "print(\"Final hidden state:\", hidden_states[-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ls3gIPU5dcEn",
        "outputId": "6b58c49a-fae5-4626-aebc-374fb4c58bd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outputs: [array([[0.00027748],\n",
            "       [0.00016316]]), array([[0.00024286],\n",
            "       [0.00024122]]), array([[3.85212746e-05],\n",
            "       [9.64918783e-05]]), array([[ 3.26208925e-06],\n",
            "       [-2.08332242e-04]])]\n",
            "Final hidden state: [[-0.05627065]\n",
            " [-0.01653509]\n",
            " [-0.01294208]\n",
            " [-0.00424558]\n",
            " [-0.05891212]]\n"
          ]
        }
      ]
    }
  ]
}